{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đầu vào: Chia sẻ tâm hồn và quà tặng cuộc sống Trần Bình\n",
      "Tác giả: sống Trần Bình\n",
      "Tên tài liệu: Chia sẻ tâm hồn và quà tặng cuộc\n",
      "Nhà xuất bản: \n",
      "Năm xuất bản:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "lowest_prob_accepted = 0.2\n",
    "def no_accent_vietnamese(s):\n",
    "    s = re.sub(u'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
    "    s = re.sub(u'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪ]', 'A', s)\n",
    "    s = re.sub(u'[èéẹẻẽêềếệểễ]', 'e', s)\n",
    "    s = re.sub(u'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
    "    s = re.sub(u'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
    "    s = re.sub(u'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', s)\n",
    "    s = re.sub(u'[ìíịỉĩ]', 'i', s)\n",
    "    s = re.sub(u'[ÌÍỊỈĨ]', 'I', s)\n",
    "    s = re.sub(u'[ùúụủũưừứựửữ]', 'u', s)\n",
    "    s = re.sub(u'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
    "    s = re.sub(u'[ỳýỵỷỹ]', 'y', s)\n",
    "    s = re.sub(u'[ỲÝỴỶỸ]', 'Y', s)\n",
    "    s = re.sub(u'[Đ]', 'D', s)\n",
    "    s = re.sub(u'[đ]', 'd', s)\n",
    "    return s\n",
    "\n",
    "def get_all_substrings(input_string_array):\n",
    "    length = len(input_string_array)\n",
    "    return [input_string[i:j + 1] for i in range(length) for j in range(i, length)]\n",
    "\n",
    "def author_traindata():\n",
    "    authorFilepath = './traindata/TacGia.txt'  \n",
    "    authorF = io.open(authorFilepath, mode=\"r\", encoding=\"utf-8\")\n",
    "    authors = []\n",
    "    for i, line in enumerate(authorF):\n",
    "        data = line.strip().replace(u'\\ufeff', '')\n",
    "        authors.append(data)\n",
    "        data1 = 'Tác giả ' + data\n",
    "        authors.append(data1)\n",
    "        data = no_accent_vietnamese(data)\n",
    "        authors.append(data)\n",
    "    return authors\n",
    "\n",
    "def docName_traindata():\n",
    "    docFilepath = './traindata/TenTL.txt'  \n",
    "    docF = io.open(docFilepath, mode=\"r\", encoding=\"utf-8\")\n",
    "    docNames = []\n",
    "    for i, line in enumerate(docF):\n",
    "        data = line.strip().replace(u'\\ufeff', '')\n",
    "        docNames.append(data)\n",
    "        data1 = 'Giáo trình ' + data\n",
    "        docNames.append(data1)\n",
    "        data2 = 'Lý thuyết ' + data\n",
    "        docNames.append(data2)\n",
    "        data3 = 'Bài tập ' + data\n",
    "        docNames.append(data3)\n",
    "        data = no_accent_vietnamese(data)\n",
    "        docNames.append(data)\n",
    "    return docNames\n",
    "\n",
    "def publisher_traindata():\n",
    "    publisherFilepath = './traindata/NXB.txt'  \n",
    "    publisherF = io.open(publisherFilepath, mode=\"r\", encoding=\"utf-8\")\n",
    "    publishers = []\n",
    "    for i, line in enumerate(publisherF):\n",
    "        data = line.strip().replace(u'\\ufeff', '')\n",
    "        publishers.append(data)\n",
    "        data3 = 'Đại học ' + data\n",
    "        publishers.append(data3)  \n",
    "        data = no_accent_vietnamese(data)\n",
    "        publishers.append(data)  \n",
    "    return publishers\n",
    "\n",
    "def another(i):\n",
    "    if i == 0:\n",
    "        return 1\n",
    "    if i == 1:\n",
    "        return 0\n",
    "    if i == 2:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "def the_other(i):\n",
    "    if i == 0:\n",
    "        return 2\n",
    "    if i == 1:\n",
    "        return 2\n",
    "    if i == 2:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def get_classified_words(classifier, all_possible_strings, which):\n",
    "    i = another(which)\n",
    "    j = the_other(which)\n",
    "    firstTime = True\n",
    "    max_vote = 0\n",
    "    highest_voted_string = ''\n",
    "    for string in all_possible_strings:\n",
    "        if len(string) > 1:\n",
    "            string = ' '.join(string)\n",
    "            X_test = np.array([string]) \n",
    "            vote = classifier.decision_function(X_test)\n",
    "            if firstTime:\n",
    "                firstTime = False\n",
    "                max_vote = vote[0][which]\n",
    "                highest_voted_string = string\n",
    "            else:\n",
    "                if vote[0][which] > max_vote: \n",
    "                    max_vote = vote[0][which]\n",
    "                    highest_voted_string = string\n",
    "    if max_vote < lowest_prob_accepted:\n",
    "        return ''\n",
    "    return highest_voted_string\n",
    "\n",
    "\n",
    "authors = author_traindata()\n",
    "docNames = docName_traindata()\n",
    "publishers = publisher_traindata()\n",
    "\n",
    "X_train_pre = authors + docNames + publishers\n",
    "X_train = np.array(X_train_pre)\n",
    "\n",
    "y_train = []\n",
    "for x in authors:\n",
    "    y_train.append(0)\n",
    "for x in docNames:\n",
    "    y_train.append(1)\n",
    "for x in publishers:\n",
    "    y_train.append(2)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(C=10))])\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#input_string = 'Nguyễn Thị Cẩm Uyên Quà tặng cuộc sống 2 Bách Khoa Hà Nội 2017'\n",
    "#input_string = 'Bài tập Xác suất thống kê 9 Nguyễn Thanh Loan NXB Đà Nẵng 2001'\n",
    "#input_string = 'Nguyễn Đức Nghĩa Cấu trúc dữ liệu và thuật toán Bách Khoa Hà Nội 2013'\n",
    "#input_string = 'NXB Đà Nẵng 2009 Bài tập Tiếng Anh 10 Hà Thanh Uyên'\n",
    "#input_string = '2016 Tâm lý học trong nháy mắt Phạm Huy Hoàng Thời đại'\n",
    "#input_string = 'Mai Lan Hương Bài tập trắc nghiệm Tiếng Anh 10 Đà Nẵng'\n",
    "#input_string = 'Mai Lan Hương 2008 Giải tích 1'\n",
    "input_string = 'Hóa học đại cương Lương Duyên Bình Bách Khoa Hà Nội 2003'\n",
    "print('Đầu vào:', input_string)\n",
    "\n",
    "# Input Preprocessing\n",
    "input_string = input_string.replace('NXB', '')\n",
    "input_string = input_string.replace('Nhà xuất bản', '')\n",
    "input_string = input_string.replace('nhà xuất bản', '')\n",
    "\n",
    "match = re.search('(?:(?<!\\d)\\d{4}(?!\\d))', input_string)\n",
    "pubYear = 0\n",
    "if match != None:\n",
    "    pubYear = match.group(0)\n",
    "    input_string = input_string.replace(pubYear, '')\n",
    "    pubYear = int(pubYear)\n",
    "    now = datetime.datetime.now()\n",
    "    if (pubYear < 1700 or pubYear > now.year):\n",
    "        pubYear = 0\n",
    "match = re.search(' \\d\\d', input_string)\n",
    "docNumber = 0\n",
    "if match != None:\n",
    "    docNumber = match.group(0)\n",
    "    input_string = input_string.replace(docNumber, '')\n",
    "    docNumber = int(docNumber.strip())\n",
    "else:\n",
    "    match = re.search(' \\d', input_string)\n",
    "    docNumber = 0\n",
    "    if match != None:\n",
    "        docNumber = match.group(0)\n",
    "        input_string = input_string.replace(docNumber, '')\n",
    "        docNumber = int(docNumber.strip())\n",
    "input_string = input_string.strip()\n",
    "\n",
    "original_string = input_string\n",
    "input_string = input_string.split(' ')\n",
    "all_possible_strings = get_all_substrings(input_string)\n",
    "output = [''] * 3\n",
    "output[0] = get_classified_words(classifier, all_possible_strings, 0).strip()\n",
    "output[1] = get_classified_words(classifier, all_possible_strings, 1).strip()\n",
    "output[2] = get_classified_words(classifier, all_possible_strings, 2).strip()\n",
    "\n",
    "def output_tuning(classifier, original_string, output):\n",
    "    first_pos = min([original_string.index(output[0]), original_string.index(output[1]), original_string.index(output[2])])\n",
    "    middle_pos = -1\n",
    "    last_pos = max([original_string.index(output[0]), original_string.index(output[1]), original_string.index(output[2])])\n",
    "    first = -1\n",
    "    middle = -1\n",
    "    last = -1\n",
    "    for i in range(0, 3):\n",
    "        pos = original_string.index(output[i])\n",
    "        if pos == first_pos:\n",
    "            first = i\n",
    "        else:\n",
    "            if pos == last_pos:\n",
    "                last = i            \n",
    "    for i in range(0, 3):\n",
    "        if i != first and i != last:\n",
    "            middle = i\n",
    "            middle_pos = original_string.index(output[middle])\n",
    "            break\n",
    "    if output[first] != None and output[first] != '':\n",
    "        if first_pos > 0:\n",
    "            output[first] = original_string[:first_pos] + output[first]\n",
    "            first_pos = 0\n",
    "    if output[last] != None and output[last] != '':\n",
    "        if last_pos + len(output[last]) != len(original_string):\n",
    "            output[last] = output[last] + original_string[last_pos + len(output[last]):]\n",
    "    if first_pos + len(output[first]) != middle_pos:\n",
    "        output_first = output[first] + original_string[first_pos + len(output[first]):middle_pos]\n",
    "        output_first = output_first.strip()\n",
    "        output_first_prob = classifier.decision_function(np.array([output_first]))[0][first]\n",
    "        output_middle = original_string[first_pos + len(output[first]):middle_pos] + output[middle]\n",
    "        output_middle = output_middle.strip()\n",
    "        output_middle_prob = classifier.decision_function(np.array([output_middle]))[0][middle]\n",
    "        if (output_first_prob > output_middle_prob):\n",
    "            output[first] = output_first\n",
    "        else:\n",
    "            output[middle] = output_middle\n",
    "            middle_pos = first_pos + len(output[first]) + 1\n",
    "    if middle_pos + len(output[middle]) != last_pos:\n",
    "        output_middle = output[middle] + original_string[middle_pos + len(output[middle]):last_pos]\n",
    "        output_middle = output_middle.strip()\n",
    "        output_middle_prob = classifier.decision_function(np.array([output_middle]))[0][middle]\n",
    "        output_last = original_string[middle_pos + len(output[middle]) + 1:last_pos] + output[last]\n",
    "        output_last = output_last.strip()\n",
    "        output_last_prob = classifier.decision_function(np.array([output_last]))[0][last]\n",
    "        if (output_middle_prob > output_last_prob):\n",
    "            output[middle] = output_middle\n",
    "        else:\n",
    "            output[last] = output_last\n",
    "    return output\n",
    "\n",
    "countNone = 0\n",
    "if output[0] == '':\n",
    "    countNone += 1\n",
    "if output[1] == '':\n",
    "    countNone += 1\n",
    "if output[2] == '':\n",
    "    countNone += 1\n",
    "if countNone < 2:\n",
    "    output = output_tuning(classifier, original_string, output)\n",
    "    print('Tác giả:', output[0])\n",
    "    if docNumber != 0 and output[0] != '':\n",
    "        print('Tên tài liệu:', output[1], docNumber)\n",
    "    else:\n",
    "        print('Tên tài liệu:', output[1])\n",
    "    print('Nhà xuất bản:', output[2])\n",
    "else:\n",
    "    target_names = ['Tác giả', 'Tên tài liệu', 'Nhà xuất bản']\n",
    "    predicted = classifier.predict(np.array([original_string]))\n",
    "    if predicted[0] != 1:\n",
    "        print(target_names[predicted[0]], ': ', original_string, sep='')\n",
    "    else:\n",
    "        if docNumber != 0:\n",
    "            print('Tên tài liệu:', original_string, docNumber)\n",
    "        else:\n",
    "            print('Tên tài liệu:', original_string)\n",
    "if pubYear != 0:\n",
    "    print('Năm xuất bản:', pubYear)\n",
    "else:\n",
    "    print('Năm xuất bản:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
